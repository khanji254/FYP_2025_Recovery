\section{Testing and Validation Methodology}
\label{sec:testing}

A comprehensive testing and validation plan will be implemented to evaluate system performance and verify that design objectives are achieved. Testing will be conducted in phases, progressing from component-level validation to full system integration testing.

\subsection{Component-Level Testing}

\subsubsection{Mechanical Subsystem Testing}

\textbf{Vibrating Feeder Characterization:} The vibrating feeder will be tested to determine optimal amplitude and frequency settings that produce consistent single-file berry flow without excessive bouncing or overlap. Flow rate measurements will be conducted at various vibration levels to establish the operating range.

\textbf{Chute Alignment Verification:} Berry trajectory consistency will be verified by tracking berry positions across multiple passes. High-speed video analysis will be used to confirm that berries maintain predictable paths through the imaging zone.

\textbf{Mechanical Vibration Isolation:} Accelerometers will be mounted on the camera holder and imaging zone to measure vibration transmission from the feeder. Isolation effectiveness will be quantified by comparing vibration amplitudes with and without damping mechanisms.

\subsubsection{Electrical and Control System Testing}

\textbf{Co-processor Communication Latency:} Serial communication latency between the Raspberry Pi and co-processing unit will be measured under various data transfer rates. Target latency should remain below 10 ms to maintain real-time performance.

\textbf{Air Ejector Response Time:} Solenoid valve actuation time (from trigger signal to full air blast) will be characterized using high-speed video and pressure sensors. Response time consistency will be verified across repeated actuations.

\textbf{Power Supply Stability:} Voltage and current measurements will be taken during peak load conditions (simultaneous image capture, processing, and actuation) to ensure adequate power delivery and minimal voltage sag.

\subsubsection{Image Acquisition and Processing Testing}

\textbf{Lighting Uniformity Analysis:} Light intensity will be measured across the imaging zone using a lux meter to ensure uniform illumination. Shadows and hotspots will be identified and mitigated through LED placement adjustments.

\textbf{Motion Blur Assessment:} Images will be captured at various conveyor speeds to determine the threshold speed at which motion blur degrades classification performance. Shutter speed and exposure adjustments will be tested as mitigation strategies.

\textbf{Classification Model Validation:} The trained binary classification model (ripe vs. unripe) will be evaluated using a hold-out test dataset not seen during training. Performance metrics including accuracy, precision, recall, and F1-score will be calculated to assess model robustness.

\subsection{System Integration Testing}

\subsubsection{End-to-End Sorting Performance Evaluation}

A standardized test procedure will be developed to evaluate the complete sorting system under controlled conditions:

\begin{enumerate}
    \item Prepare test batches containing pre-counted and labeled coffee berries with known ripe/unripe distributions
    \item Configure system settings (conveyor speed, camera parameters, ejector timing)
    \item Load test batch into hopper and initiate sorting process
    \item Collect sorted berries from accept and reject streams
    \item Count and classify collected berries manually to determine ground truth
    \item Calculate performance metrics: sorting accuracy, false accept rate, false reject rate, throughput (kg/hr)
    \item Repeat testing for multiple batches to establish performance consistency
    \item Document failure modes and optimization opportunities
\end{enumerate}

Target performance metrics:
\begin{itemize}
    \item Sorting accuracy: $>$ 90\% (correctly classified berries / total berries)
    \item Throughput: 150-200 kg/hr (2.5x improvement over baseline 50-75 kg/hr)
    \item False accept rate (unripe berries in accept stream): $<$ 8\%
    \item False reject rate (ripe berries in reject stream): $<$ 12\%
    \item System uptime: $>$ 95\% during 4-hour continuous operation test
\end{itemize}

\subsubsection{Stress Testing and Reliability Assessment}

\textbf{Extended Operation Testing:} The system will be operated continuously for 4-8 hours using realistic berry volumes to assess mechanical wear, thermal stability, and sustained performance levels. Any degradation in accuracy or throughput over time will be documented.

\textbf{Environmental Variation Testing:} System performance will be evaluated under varying ambient lighting conditions, temperature ranges, and humidity levels to verify robustness in real-world agricultural processing environments.

\textbf{Component Failure Mode Analysis:} Deliberate introduction of fault conditions (e.g., compressed air pressure drop, camera obstruction, processor overload) will be used to assess system resilience and identify failure modes requiring additional safeguards.

\subsection{Data Collection and Analysis}

Throughout testing, the following data will be systematically collected:

\begin{itemize}
    \item Processing frame rates and latency measurements
    \item Classification confidence scores for each detected berry
    \item Ejector actuation timing and success rates
    \item System error logs and exception handling events
    \item Power consumption profiles during various operating modes
\end{itemize}

Statistical analysis will be performed on collected data to identify performance trends, correlations between system parameters and sorting accuracy, and areas requiring further optimization. Results will be documented in detailed test reports with supporting charts, tables, and visual evidence.
